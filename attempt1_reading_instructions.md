# AI Policy Course Reading Audit – Attempt 1 (Comprehensive)

This attempt retains the full breadth of the original curriculum but strengthens guidance for each reading and suggests updates where necessary. For each source below you will find a brief summary, the **original instruction** from the curriculum, a **revised instruction** tailored for learners in 2025, and any **alternative readings** to consider if the original is outdated or inaccessible.

## 1. X‑Risk Analysis for AI Research (arXiv:2206.05862)

**Summary:**  
This paper introduces a taxonomy of existential risks from advanced AI, including misalignment, misuse by bad actors, structural risks (e.g. extreme inequality), and systemic “unknown unknowns.” It sets the stage for technical alignment debates.

**Original instruction:** read pp. 1–9.

**Revised instruction:**
- **Deep read:** Introduction and risk taxonomy (pp. 1–3) to understand each risk category.  
- **Skim:** Technical discussion of probability estimates (pp. 4–6), focusing on the assumptions behind the models.  
- **Deep read:** Conclusion (pp. 7–9) where the authors recommend areas for further research.  
- **Reflection:** Identify which risk categories you find most concerning and list two mitigation strategies for each.

**Alternative suggestions:** None needed—this is a foundational framing. However, see Carlsmith’s 2022 report “Is power-seeking AI an existential risk?” for a recent complementary perspective.

## 2. Concrete Problems in AI Safety (Revisited) (arXiv:2401.10899)

**Summary:**  
An updated catalogue of specification‑gaming behaviours and technical misalignment issues (reward hacking, negative side effects, robustness to distributional shift). Provides examples from large language models and reinforcement learning.

**Original instruction:** read all.

**Revised instruction:**
- **Deep read:** High-level overview and problem list (pp. 1–8) to familiarise yourself with each safety problem.  
- **Skim:** Detailed formal proofs and experimental setups (pp. 9–14).  
- **Deep read:** Recommendations for research directions (final section).  
- **Exercise:** Pick one problem and outline how it might manifest in a current generative model; propose one technical mitigation.

**Alternative suggestions:** Pair this with the Alignment Research Center’s “Scheming AIs” paper for a behavioural perspective.

## 3. How Persuasive is AI‑Generated Propaganda? (CSET, 2023)

**Summary:**  
Reports results from controlled experiments showing that AI‑generated propaganda can be almost as persuasive as human‑written content when readers are unaware of its origin. Includes methodology and policy recommendations.

**Original instruction:** not specified (read full piece).

**Revised instruction:**
- **Deep read:** Abstract, methodology overview, and results sections.  
- **Skim:** Detailed statistical appendices.  
- **Discussion:** In groups, debate whether disclosure requirements for AI‑generated content would meaningfully reduce persuasion. What additional safeguards (media literacy, platform labelling) are needed?

**Alternative suggestions:** Also review CSET’s companion piece “Detecting AI‑Generated Propaganda” (2024) for methods to counter these narratives.

## 4. State of Cyberwarfare 2024 (Armis report)

**Summary:**  
Provides an industry‑driven overview of cyber threats, emphasising AI‑enabled malware, phishing, and deep‑fake–assisted social engineering. Identifies sectors (healthcare, critical infrastructure) that have seen increased attacks.

**Original instruction:** none given—contextual reading.

**Revised instruction:**
- **Skim:** Executive summary for trends and key statistics.  
- **Deep read:** Sections on AI‑driven attack vectors (pp. 10–22) and recommended defensive measures.  
- **Optional:** Sector‑specific case studies if relevant to your capstone project.  
- **Think:** Are there governance levers (standards, liability regimes) that could reduce the prevalence of AI‑assisted cyberattacks?

**Alternative suggestions:** For an academic viewpoint, see “Securing AI Model Weights” (RAND, 2024) if accessible; this examines protecting model artefacts themselves.

## 5. EU AI Act – Article 57: Regulatory Sandboxes

**Summary:**  
Article 57 of the EU AI Act describes how member states can establish regulatory sandboxes that allow developers to test high‑risk AI systems in a controlled environment under regulator supervision. It specifies eligibility, obligations, and oversight requirements.

**Original instruction:** read article together with complementary sandbox readings.

**Revised instruction:**
- **Deep read:** Paragraphs 1–6 outlining the purpose, scope, and participant obligations.  
- **Skim:** Administrative provisions (paragraphs 7–9).  
- **Prepare:** Draft three questions you would ask a regulator when applying for a sandbox (e.g. liability, data requirements).  
- **Critique:** Compare the EU sandbox approach with your home jurisdiction’s experimentation environment (if any).

**Alternative suggestions:** None needed; but consult the OECD’s 2023 “Regulatory Sandboxes in Artificial Intelligence” survey for international comparisons.

## 6. The EU AI Act Will Have Global Impact—but a Limited Brussels Effect (Brookings, 2024)

**Summary:**  
Argues that while the EU AI Act sets a de facto global standard for high‑risk AI practices (e.g. conformity assessment, prohibited uses), its extraterritorial impact will be moderated by enforcement capacity and divergence in U.S. and Chinese regimes.

**Original instruction:** not specified; contextual background.

**Revised instruction:**
- **Deep read:** Analysis of high‑risk system obligations and global spill‑over effects.  
- **Skim:** Political background on EU legislative bargaining.  
- **Debate:** Do you agree with the author that the “Brussels effect” will be limited? Identify two factors that strengthen or weaken EU normative power.

**Alternative suggestions:** See Plurus Strategies’ 2024 comparison of EU, U.S. and Chinese AI policies for a tabular perspective.

## 7. Computing Power and the Governance of AI (GovAI, 2022)

**Summary:**  
Explores how control of cutting‑edge chips, large data centres, and cloud infrastructure can serve as governance levers for AI safety. Proposes international coordination on export controls and compute tracing.

**Original instruction:** read pp. 2–7, 24–33, 60–72.

**Revised instruction:**
- **Deep read:** Sections on why compute concentration matters (pp. 2–7) and proposed policy tools (pp. 24–33).  
- **Skim:** Historical case studies (pp. 60–72) unless you are unfamiliar with chip export controls.  
- **Discuss:** Should compute be treated like a dual‑use commodity? What verification mechanisms would be necessary for a global compute governance regime?

**Alternative suggestions:** For a more up‑to‑date view on chip geopolitics, see e.g. China’s 2024 chip offensive analysis (Archive.ph snapshot) if accessible.

## 8. Freedom of Speech and AI Output (Journal of Free Speech Law, 2023)

**Summary:**  
Examines whether AI‑generated speech is protected under U.S. free‑speech doctrine. Considers the status of generative models as speakers, intermediary liability, and the applicability of Section 230.

**Original instruction:** none given.

**Revised instruction:**
- **Deep read:** Introduction and analysis of existing jurisprudence.  
- **Skim:** Comparative law discussions if you are not specialising in U.S. law.  
- **Reflect:** Under what circumstances should AI‑generated content be regulated like human speech? Should different liability standards apply to developers versus users?

**Alternative suggestions:** Combine with Lawfare’s “Section 230 Won’t Protect ChatGPT” for a journalistic perspective on intermediary liability.

## 9. Copyright Policy Options for Generative AI (CEPR VoxEU, 2023)

**Summary:**  
Outlines possible copyright regimes for AI‑generated works, including registering training data, levies, and mandatory licensing. Evaluates trade‑offs between innovation and protection of creators.

**Original instruction:** none given; optional reading.

**Revised instruction:**
- **Deep read:** Proposed policy options and their legal justifications.  
- **Skim:** Economic modelling sections unless you have a law/economics background.  
- **Discuss:** Which option best balances innovation and creators’ rights? Suggest one additional mechanism not considered by the authors.

**Alternative suggestions:** For a legal case study, read Andersen v. Stability AI (ItsArtLaw, 2024) to see how copyright disputes play out in court.

## 10. The Windfall Clause: Distributing the Benefits of AI (FHI Oxford, 2017)

**Summary:**  
Proposes a voluntary commitment by AI developers to share a portion of extreme profits (“windfalls”) with society, akin to a charitable trust. Discusses enforcement mechanisms and distribution schemes.

**Original instruction:** none given.

**Revised instruction:**
- **Deep read:** Conceptual framework and proposed distribution formula.  
- **Skim:** Historical analogues (e.g. patents).  
- **Consider:** Is a windfall clause politically feasible? What international organisations could administer such a mechanism?

**Alternative suggestions:** More recent discussions on “windfall taxes” in AI could complement this (see FHI updates or OECD reports).

---

This comprehensive attempt keeps most original readings but strengthens the scaffolding: it breaks down page ranges, offers reflection prompts, and flags complementary readings. A learner following this guide should gain a deep yet manageable understanding of the AI policy landscape while being aware of emerging alternatives.
